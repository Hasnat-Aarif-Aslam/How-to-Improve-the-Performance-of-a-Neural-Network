{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wn9cuuvb_2Au"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "from keras import layers\n",
        "import keras_tuner as kt\n",
        "import numpy as np\n",
        "\n",
        "# Define a function that builds the model\n",
        "def build_model(hp):\n",
        "    model = keras.Sequential()\n",
        "    model.add(layers.Flatten(input_shape=(28, 28, 1)))  # Adjust input shape based on your dataset\n",
        "\n",
        "    # Tune the number of layers\n",
        "    for i in range(hp.Int(\"num_layers\", 1, 5)):\n",
        "        # Tune the number of units separately for each layer\n",
        "        model.add(\n",
        "            layers.Dense(\n",
        "                units=hp.Int(f\"units_{i}\", min_value=32, max_value=512, step=32),\n",
        "                activation=hp.Choice(f\"activation_{i}\", [\"relu\", \"tanh\", \"sigmoid\", \"leaky_relu\"]),\n",
        "            )\n",
        "        )\n",
        "\n",
        "        # Tune batch normalization after each layer\n",
        "        if hp.Boolean(f\"batch_norm_{i}\"):\n",
        "            model.add(layers.BatchNormalization())\n",
        "\n",
        "        # Tune whether to use dropout after each layer\n",
        "        if hp.Boolean(f\"dropout_{i}\"):\n",
        "            model.add(layers.Dropout(rate=hp.Float(f\"dropout_rate_{i}\", 0.0, 0.5, step=0.1)))\n",
        "\n",
        "    # Output layer (adjust number of units to your number of classes)\n",
        "    model.add(layers.Dense(10, activation=\"softmax\"))  # Modify this line if you have more than 10 classes\n",
        "\n",
        "    # Tune the optimizer type and its learning rate\n",
        "    optimizer = hp.Choice('optimizer', ['adam', 'rmsprop', 'sgd', 'adagrad', 'nadam'])\n",
        "\n",
        "    if optimizer == 'adam':\n",
        "        chosen_optimizer = keras.optimizers.Adam(\n",
        "            learning_rate=hp.Float(\"learning_rate\", min_value=1e-4, max_value=1e-2, sampling=\"log\")\n",
        "        )\n",
        "    elif optimizer == 'rmsprop':\n",
        "        chosen_optimizer = keras.optimizers.RMSprop(\n",
        "            learning_rate=hp.Float(\"learning_rate\", min_value=1e-4, max_value=1e-2, sampling=\"log\")\n",
        "        )\n",
        "    elif optimizer == 'sgd':\n",
        "        chosen_optimizer = keras.optimizers.SGD(\n",
        "            learning_rate=hp.Float(\"learning_rate\", min_value=1e-4, max_value=1e-2, sampling=\"log\"),\n",
        "            momentum=hp.Float(\"momentum\", min_value=0.0, max_value=0.9, step=0.1)\n",
        "        )\n",
        "    elif optimizer == 'adagrad':\n",
        "        chosen_optimizer = keras.optimizers.Adagrad(\n",
        "            learning_rate=hp.Float(\"learning_rate\", min_value=1e-4, max_value=1e-2, sampling=\"log\")\n",
        "        )\n",
        "    elif optimizer == 'nadam':\n",
        "        chosen_optimizer = keras.optimizers.Nadam(\n",
        "            learning_rate=hp.Float(\"learning_rate\", min_value=1e-4, max_value=1e-2, sampling=\"log\")\n",
        "        )\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=chosen_optimizer,\n",
        "        loss=\"categorical_crossentropy\",\n",
        "        metrics=[\"accuracy\"],\n",
        "    )\n",
        "    return model\n",
        "\n",
        "# Initialize the tuner\n",
        "tuner = kt.RandomSearch(\n",
        "    hypermodel=build_model,\n",
        "    objective=\"val_accuracy\",\n",
        "    max_trials=20,  # You can increase the number of trials to explore more configurations\n",
        "    executions_per_trial=2,\n",
        "    overwrite=True,\n",
        "    directory=\"my_dir\",\n",
        "    project_name=\"tune_everything\",\n",
        ")\n",
        "\n",
        "# Print the search space summary\n",
        "tuner.search_space_summary()\n",
        "\n",
        "# Prepare your dataset (Replace this with your dataset)\n",
        "(x_train, y_train), (x_val, y_val) = keras.datasets.mnist.load_data()\n",
        "x_train = np.expand_dims(x_train, -1).astype(\"float32\") / 255.0\n",
        "x_val = np.expand_dims(x_val, -1).astype(\"float32\") / 255.0\n",
        "\n",
        "# Ensure y_train and y_val are one-hot encoded\n",
        "num_classes = 10  # Adjust this to match the number of classes in your dataset\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_val = keras.utils.to_categorical(y_val, num_classes)\n",
        "\n",
        "# Start the search for the best hyperparameter configuration\n",
        "tuner.search(x_train, y_train, epochs=10, validation_data=(x_val, y_val))\n",
        "\n",
        "# Get the best model\n",
        "best_model = tuner.get_best_models(num_models=1)[0]\n",
        "best_model.summary()\n",
        "\n",
        "# Optionally, you can retrain the best model on the entire dataset\n",
        "x_all = np.concatenate((x_train, x_val))\n",
        "y_all = np.concatenate((y_train, y_val))\n",
        "best_model.fit(x_all, y_all, epochs=10)"
      ]
    }
  ]
}